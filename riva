import onnxruntime as ort
import numpy as np
import torch
import torchaudio
from transformers import WhisperProcessor

# ==============================
# 1. Load Whisper Processor (Tokenizer + Feature Extractor)
# ==============================
model_name = "openai/whisper-large-v3"
processor = WhisperProcessor.from_pretrained(model_name)

# ==============================
# 2. TensorRT Providers Configuration
# ==============================
providers = [
    (
        "TensorrtExecutionProvider",
        {
            "trt_engine_cache_enable": True,
            "trt_engine_cache_path": "./trt_cache",
            "trt_fp16_enable": True,   # Enable FP16 for speed
            "trt_int8_enable": False,
            "trt_max_workspace_size": 2 * 1024 * 1024 * 1024,  # 2GB
        },
    ),
    ("CUDAExecutionProvider", {}),
    ("CPUExecutionProvider", {}),
]

# ==============================
# 3. Load Encoder & Decoder ONNX Sessions
# ==============================
encoder_sess = ort.InferenceSession("whisper_encoder.onnx", providers=providers)
decoder_sess = ort.InferenceSession("whisper_decoder.onnx", providers=providers)

print("Encoder running on:", encoder_sess.get_providers())
print("Decoder running on:", decoder_sess.get_providers())

# ==============================
# 4. Preprocess Audio
# ==============================
def load_audio(file_path):
    waveform, sr = torchaudio.load(file_path)
    if sr != 16000:
        waveform = torchaudio.functional.resample(waveform, sr, 16000)
    return waveform.squeeze()

audio_path = "sample.wav"  # <-- Replace with your audio file
audio = load_audio(audio_path)

# Convert to Whisper's log-mel spectrogram
inputs = processor(audio, sampling_rate=16000, return_tensors="np")
mel = inputs.input_features  # shape: (1, 80, T)

# ==============================
# 5. Run Encoder (TensorRT)
# ==============================
encoder_out = encoder_sess.run(None, {"input_features": mel})[0]

# ==============================
# 6. Greedy Decoding (ONNX Decoder)
# ==============================
# Whisper uses autoregressive decoding. For simplicity, we do greedy search.
# For better accuracy, implement beam search separately.

# Initialize decoder inputs
decoder_input_ids = np.array([[processor.tokenizer.lang_code_to_id["<|en|>"]]])  # English
encoder_out_np = encoder_out

max_length = 100
eos_token_id = processor.tokenizer.eos_token_id
generated_ids = []

for _ in range(max_length):
    outputs = decoder_sess.run(
        None,
        {
            "input_ids": decoder_input_ids.astype(np.int64),
            "encoder_hidden_states": encoder_out_np.astype(np.float32),
        },
    )
    logits = outputs[0][:, -1, :]  # last token logits
    next_token = np.argmax(logits, axis=-1)
    generated_ids.append(next_token.item())
    
    if next_token == eos_token_id:
        break
    
    decoder_input_ids = np.hstack([decoder_input_ids, next_token.reshape(1, 1)])

# ==============================
# 7. Decode to Text
# ==============================
transcription = processor.tokenizer.decode(generated_ids, skip_special_tokens=True)
print("Transcription:", transcription)
